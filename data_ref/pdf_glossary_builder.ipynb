{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26a9189f",
   "metadata": {},
   "source": [
    "# PDF Glossary Builder\n",
    "\n",
    "このノートブックは `data_ref` 配下の PDF を順次読み込み、資料ごとの固有用語と定義を抽出した JSON (`glossary_terms.json`) を生成する PoC です。`.env` に OpenAI API キーを設定し、上から順に実行してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc65b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Import dependencies and declare global configuration.\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Sequence\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from pypdf import PdfReader\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# DATA_REF_DIR = Path(\"data_ref\")\n",
    "DATA_REF_DIR = Path(\"\")\n",
    "OUTPUT_JSON_PATH = DATA_REF_DIR / \"glossary_terms.json\"\n",
    "PDF_GLOSSARY_MODEL = os.getenv(\"PDF_GLOSSARY_MODEL\", \"gpt-4o-mini\")\n",
    "MAX_CHARS_PER_CHUNK = int(os.getenv(\"PDF_GLOSSARY_MAX_CHARS\", \"1200\"))\n",
    "CHUNK_OVERLAP = int(os.getenv(\"PDF_GLOSSARY_CHUNK_OVERLAP\", \"200\"))\n",
    "MAX_TERMS_PER_CHUNK = int(os.getenv(\"PDF_GLOSSARY_TERMS_PER_CHUNK\", \"4\"))\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise EnvironmentError(\"OPENAI_API_KEY is not set. Populate it in .env before running this notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d70ee47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Define structured outputs, prompts, and PDF/chunk helper functions.\n",
    "class GlossaryEntry(BaseModel):\n",
    "    term: str = Field(..., description=\"専門用語 (短いラベル)\")\n",
    "    definition: str = Field(..., description=\"資料の内容に基づく日本語定義\")\n",
    "    source_pages: List[int] = Field(..., description=\"定義を裏付けるページ番号 (1-based)\")\n",
    "    confidence: float = Field(..., ge=0, le=1, description=\"モデルの自己評価 (0-1)\")\n",
    "\n",
    "\n",
    "class GlossaryBatch(BaseModel):\n",
    "    entries: List[GlossaryEntry]\n",
    "\n",
    "\n",
    "glossary_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a meticulous Japanese technical editor. \"\n",
    "        \"Identify domain-specific terms that appear unique to the provided document excerpt. \"\n",
    "        \"Only output terms that can be clearly defined using the excerpt. \"\n",
    "        \"Respond in JSON via the supplied schema.\",\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"Document: {doc_name}\\n\"\n",
    "        \"Pages: {page_span}\\n\"\n",
    "        \"Max terms allowed: {max_terms}\\n\"\n",
    "        \"Excerpt:\\n{excerpt}\\n\"\n",
    "        \"\\nConstraints:\\n\"\n",
    "        \"- Prefer名詞やカタカナ語など資料固有のキーワード\\n\"\n",
    "        \"- definitionは資料内の根拠を説明する日本語\\n\"\n",
    "        \"- source_pagesは整数のみ\\n\"\n",
    "        \"- confidenceは0.0~1.0で相対的な確信度\",\n",
    "    ),\n",
    "])\n",
    "\n",
    "structured_llm = ChatOpenAI(\n",
    "    model=PDF_GLOSSARY_MODEL,\n",
    "    temperature=0,\n",
    "    api_key=OPENAI_API_KEY,\n",
    ").with_structured_output(GlossaryBatch)\n",
    "\n",
    "\n",
    "def extract_text_by_page(pdf_path: Path) -> List[tuple[int, str]]:\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    pages: List[tuple[int, str]] = []\n",
    "    for idx, page in enumerate(reader.pages, start=1):\n",
    "        try:\n",
    "            text = page.extract_text() or \"\"\n",
    "        except Exception as exc:  # pragma: no cover - PyPDF edge cases\n",
    "            print(f\"[warn] Failed to extract page {idx} of {pdf_path.name}: {exc}\")\n",
    "            text = \"\"\n",
    "        normalized = \" \".join(text.replace(\"\\u3000\", \" \").split())\n",
    "        if normalized:\n",
    "            pages.append((idx, normalized))\n",
    "    return pages\n",
    "\n",
    "\n",
    "def chunk_pages(\n",
    "    pages: Sequence[tuple[int, str]],\n",
    "    max_chars: int = MAX_CHARS_PER_CHUNK,\n",
    "    overlap: int = CHUNK_OVERLAP,\n",
    ") -> List[Dict[str, object]]:\n",
    "    if overlap >= max_chars:\n",
    "        raise ValueError(\"CHUNK_OVERLAP must be smaller than MAX_CHARS_PER_CHUNK\")\n",
    "    chunks: List[Dict[str, object]] = []\n",
    "    buffer = \"\"\n",
    "    buffer_pages: List[int] = []\n",
    "    for page_num, text in pages:\n",
    "        tagged = f\"[page={page_num}] {text.strip()} \"\n",
    "        if not buffer:\n",
    "            buffer_pages = [page_num]\n",
    "        elif buffer_pages[-1] != page_num:\n",
    "            buffer_pages.append(page_num)\n",
    "        if len(buffer) + len(tagged) > max_chars and buffer:\n",
    "            chunks.append({\"text\": buffer.strip(), \"pages\": buffer_pages.copy()})\n",
    "            buffer = buffer[-overlap:]\n",
    "            buffer_pages = buffer_pages[-1:]\n",
    "        buffer += tagged\n",
    "    if buffer.strip():\n",
    "        chunks.append({\"text\": buffer.strip(), \"pages\": buffer_pages.copy()})\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def invoke_glossary_model(doc_name: str, chunk: Dict[str, object]) -> List[GlossaryEntry]:\n",
    "    page_span = \", \".join(str(p) for p in chunk[\"pages\"]) or \"unknown\"\n",
    "    messages = glossary_prompt.format_prompt(\n",
    "        doc_name=doc_name,\n",
    "        page_span=page_span,\n",
    "        max_terms=MAX_TERMS_PER_CHUNK,\n",
    "        excerpt=chunk[\"text\"],\n",
    "    ).to_messages()\n",
    "    response: GlossaryBatch = structured_llm.invoke(messages)\n",
    "    return response.entries\n",
    "\n",
    "\n",
    "def merge_entries(entries: Iterable[GlossaryEntry]) -> Dict[str, Dict[str, object]]:\n",
    "    merged: Dict[str, Dict[str, object]] = {}\n",
    "    for entry in entries:\n",
    "        term = entry.term.strip()\n",
    "        if not term:\n",
    "            continue\n",
    "        bucket = merged.setdefault(\n",
    "            term,\n",
    "            {\n",
    "                \"definition\": entry.definition.strip(),\n",
    "                \"source_pages\": sorted(set(entry.source_pages)),\n",
    "                \"confidence\": entry.confidence,\n",
    "                \"hits\": 1,\n",
    "            },\n",
    "        )\n",
    "        if bucket[\"definition\"] != entry.definition.strip() and len(entry.definition.strip()) > len(bucket[\"definition\"]):\n",
    "            bucket[\"definition\"] = entry.definition.strip()\n",
    "        bucket[\"source_pages\"] = sorted(set(bucket[\"source_pages\"]) | set(entry.source_pages))\n",
    "        bucket[\"confidence\"] = round((bucket[\"confidence\"] * bucket[\"hits\"] + entry.confidence) / (bucket[\"hits\"] + 1), 3)\n",
    "        bucket[\"hits\"] += 1\n",
    "    for value in merged.values():\n",
    "        value.pop(\"hits\", None)\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ada0c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1-03.pdf ...\n",
      "[warn] No extractable text in 1-03.pdf. Skipping.\n",
      "Processing 1-04.pdf ...\n",
      "[warn] No extractable text in 1-04.pdf. Skipping.\n",
      "Processing 1-06.pdf ...\n",
      "[warn] No extractable text in 1-06.pdf. Skipping.\n",
      "Processing 1-0w6.pdf ...\n",
      "[warn] No extractable text in 1-0w6.pdf. Skipping.\n",
      "Processing 2-08.pdf ...\n",
      "[warn] No extractable text in 2-08.pdf. Skipping.\n",
      "Processing 2-10.pdf ...\n",
      "[warn] No extractable text in 2-10.pdf. Skipping.\n",
      "Processing 2-101.pdf ...\n",
      "[warn] No extractable text in 2-101.pdf. Skipping.\n",
      "Wrote 0 documents into glossary_terms.json\n"
     ]
    }
   ],
   "source": [
    "# Purpose: Orchestrate per-PDF glossary generation and persist JSON output.\n",
    "def build_glossary_for_pdf(pdf_path: Path) -> Dict[str, Dict[str, object]]:\n",
    "    pages = extract_text_by_page(pdf_path)\n",
    "    if not pages:\n",
    "        print(f\"[warn] No extractable text in {pdf_path.name}. Skipping.\")\n",
    "        return {}\n",
    "    chunks = chunk_pages(pages)\n",
    "    collected: List[GlossaryEntry] = []\n",
    "    for chunk in chunks:\n",
    "        entries = invoke_glossary_model(pdf_path.name, chunk)\n",
    "        collected.extend(entries)\n",
    "    merged = merge_entries(collected)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def run_directory_pipeline(pdf_dir: Path) -> Dict[str, Dict[str, Dict[str, object]]]:\n",
    "    output: Dict[str, Dict[str, Dict[str, object]]] = {}\n",
    "    for pdf_path in sorted(pdf_dir.glob(\"*.pdf\")):\n",
    "        print(f\"Processing {pdf_path.name} ...\")\n",
    "        glossary = build_glossary_for_pdf(pdf_path)\n",
    "        if glossary:\n",
    "            output[pdf_path.name] = glossary\n",
    "    return output\n",
    "\n",
    "\n",
    "glossary_payload = run_directory_pipeline(DATA_REF_DIR)\n",
    "OUTPUT_JSON_PATH.write_text(json.dumps(glossary_payload, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Wrote {len(glossary_payload)} documents into {OUTPUT_JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcec08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: Flatten glossary payload into a term -> definition dictionary with median-style merging for duplicates.\n",
    "def select_balanced_definition(entries: List[Dict[str, float | str]]) -> str:\n",
    "    \"\"\"Pick the median-length definition (ties broken by higher confidence).\"\"\"\n",
    "    sorted_entries = sorted(\n",
    "        entries,\n",
    "        key=lambda entry: (len(entry[\"definition\"]), -float(entry.get(\"confidence\", 0.0))),\n",
    "    )\n",
    "    median_idx = len(sorted_entries) // 2\n",
    "    return sorted_entries[median_idx][\"definition\"]\n",
    "\n",
    "\n",
    "def flatten_glossary(payload: Dict[str, Dict[str, Dict[str, object]]]) -> Dict[str, str]:\n",
    "    buckets: Dict[str, List[Dict[str, object]]] = {}\n",
    "    for doc_terms in payload.values():\n",
    "        for term, info in doc_terms.items():\n",
    "            definition = str(info.get(\"definition\", \"\")).strip()\n",
    "            if not definition:\n",
    "                continue\n",
    "            buckets.setdefault(term, []).append(\n",
    "                {\"definition\": definition, \"confidence\": float(info.get(\"confidence\", 0.0))}\n",
    "            )\n",
    "    flattened: Dict[str, str] = {}\n",
    "    for term, entries in buckets.items():\n",
    "        flattened[term] = select_balanced_definition(entries)\n",
    "    return flattened\n",
    "\n",
    "\n",
    "simple_glossary = flatten_glossary(glossary_payload)\n",
    "OUTPUT_JSON_PATH.write_text(json.dumps(simple_glossary, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(f\"Flattened glossary now contains {len(simple_glossary)} unique terms -> {OUTPUT_JSON_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a53101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1-100.pdf',\n",
       "  {'第1種換気': {'definition': '排気と給気を共に機械を用いて行う方法。',\n",
       "    'source_pages': [2],\n",
       "    'confidence': 0.9},\n",
       "   'アメニティー換気': {'definition': '当社主力システムであり、第1種機械換気を指す。',\n",
       "    'source_pages': [4],\n",
       "    'confidence': 0.8},\n",
       "   '冷暖房設備': {'definition': 'エアコンや床暖房など、空気の温度を調整するための設備。',\n",
       "    'source_pages': [5],\n",
       "    'confidence': 0.85},\n",
       "   '換気扇': {'definition': '空気を入れ替えるための機械設備で、給気口や屋外フードと連携して使用される。',\n",
       "    'source_pages': [3],\n",
       "    'confidence': 0.9}}),\n",
       " ('1-17.pdf',\n",
       "  {'玄関': {'definition': '住まいの入口であり、機能だけでなく印象を高める部屋として計画される空間。',\n",
       "    'source_pages': [1, 2, 3],\n",
       "    'confidence': 0.9},\n",
       "   '玄関ポーチ': {'definition': '玄関の外に設けられるスペースで、適切な広さや雨がかりにならないように計画される。',\n",
       "    'source_pages': [4, 5, 6],\n",
       "    'confidence': 0.8},\n",
       "   '土間': {'definition': '玄関に設けられる床の一部で、通行の安全性と視覚的な広がり感を大切に計画される。',\n",
       "    'source_pages': [6, 8],\n",
       "    'confidence': 0.75},\n",
       "   '動線': {'definition': '住まいの中での人の動きの流れを指し、集中させないように計画することが求められる。',\n",
       "    'source_pages': [2, 4],\n",
       "    'confidence': 0.8},\n",
       "   '玄関ドア': {'definition': 'アプローチの方向に開くことを基本とする玄関の出入り口の扉。',\n",
       "    'source_pages': [7],\n",
       "    'confidence': 0.9},\n",
       "   '間口': {'definition': '玄関や部屋の幅を示し、2.0N以上で計画することが求められる。',\n",
       "    'source_pages': [7],\n",
       "    'confidence': 0.8},\n",
       "   '玄関収納': {'definition': 'ホールからも使用できる配置の収納スペースで、靴の収納量は50足以上を目安とする。',\n",
       "    'source_pages': [8],\n",
       "    'confidence': 0.9}})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Purpose: Peek at a few simplified entries to verify the flattened structure.\n",
    "list(simple_glossary.items())[:5]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
