{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "# Text Classification PoC v2\n",
    "- SentenceTransformer: `intfloat/multilingual-e5-small`（instruction tuned）で高品質な埋め込みを取得。\n",
    "- 5-fold Stratified CV + hold-out で SVM/LogReg/MLP/LightGBM/Bagging を比較し、Optuna で主要モデルをチューニング。\n",
    "- カテゴリ名の埋め込みとのコサイン類似度や確信度モニタリング、類似問い合わせ抽出など運用要件をPoC化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Import dependencies, set constants/logging, and configure deterministic behavior for reproducibility.\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, Iterable, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from lightgbm import LGBMClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from sklearn.ensemble import BaggingClassifier, StackingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "DATA_PATH = Path(\"data/text_classification_samples_200.csv\")\n",
    "EMBED_MODEL_NAME = \"intfloat/multilingual-e5-small\"\n",
    "E5_INSTRUCTION = \"query: \"\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "pl.enable_string_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Provide data loading helpers built on top of Polars LazyFrame for scalable ingestion.\n",
    "def load_lazy_dataset(csv_path: Path) -> pl.LazyFrame:\n",
    "    \"\"\"Return a Polars LazyFrame scanning the CSV without materializing rows.\"\"\"\n",
    "    return pl.scan_csv(csv_path)\n",
    "\n",
    "\n",
    "def preview_lazyframe(lazy_frame: pl.LazyFrame, sample_size: int = 5) -> pl.DataFrame:\n",
    "    \"\"\"Collect a small sample to inspect schema/values while keeping the query lazy.\"\"\"\n",
    "    return lazy_frame.head(sample_size).collect()\n",
    "\n",
    "\n",
    "def materialize_dataset(lazy_frame: pl.LazyFrame) -> pd.DataFrame:\n",
    "    \"\"\"Materialize the LazyFrame into a Pandas DataFrame for compatibility with sklearn.\"\"\"\n",
    "    return lazy_frame.collect().to_pandas()\n",
    "\n",
    "\n",
    "def encode_labels(\n",
    "    df: pd.DataFrame,\n",
    "    label_column: str = \"category_label\",\n",
    "    new_column: str = \"category_id\",\n",
    ") -> Tuple[pd.DataFrame, preprocessing.LabelEncoder]:\n",
    "    \"\"\"Encode string labels into integers and append them as a new column.\"\"\"\n",
    "    encoded_df = df.copy()\n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    encoded_df[new_column] = encoder.fit_transform(encoded_df[label_column])\n",
    "    return encoded_df, encoder\n",
    "\n",
    "\n",
    "def summarize_categories(\n",
    "    df: pd.DataFrame,\n",
    "    category_name_col: str = \"category_name\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Return counts and ratios per category for quick EDA.\"\"\"\n",
    "    summary = (\n",
    "        df[category_name_col]\n",
    "        .value_counts()\n",
    "        .rename(\"count\")\n",
    "        .to_frame()\n",
    "        .assign(ratio=lambda frame: frame[\"count\"] / frame[\"count\"].sum())\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": category_name_col})\n",
    "    )\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Load the dataset lazily, preview it, and persist a Pandas copy with encoded labels for modeling.\n",
    "lazy_dataset = load_lazy_dataset(DATA_PATH)\n",
    "preview_lazyframe(lazy_dataset, sample_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Materialize the dataset, attach numeric labels, and summarize category balance for reference.\n",
    "records_df, label_encoder = encode_labels(materialize_dataset(lazy_dataset))\n",
    "category_summary = summarize_categories(records_df)\n",
    "category_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Define embedding helpers tailored for instruction-tuned E5 family models.\n",
    "def normalize_for_instruction(text: str, instruction: str = E5_INSTRUCTION) -> str:\n",
    "    \"\"\"Prefix text with the E5 instruction keyword to unlock better multilingual embeddings.\"\"\"\n",
    "    clean = text.strip().replace(\"\\n\", \" \")\n",
    "    return f\"{instruction}{clean}\"\n",
    "\n",
    "\n",
    "def build_embedder(model_name: str = EMBED_MODEL_NAME) -> SentenceTransformer:\n",
    "    \"\"\"Load and return a SentenceTransformer model; default is multilingual-e5-small.\"\"\"\n",
    "    return SentenceTransformer(model_name)\n",
    "\n",
    "\n",
    "def embed_texts(\n",
    "    embedder: SentenceTransformer,\n",
    "    texts: Iterable[str],\n",
    "    instruction: str = E5_INSTRUCTION,\n",
    "    batch_size: int = 32,\n",
    "    normalize_embeddings: bool = True,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Convert iterable of texts into normalized embeddings with the provided instruction prefix.\"\"\"\n",
    "    prepared = [normalize_for_instruction(text, instruction) for text in texts]\n",
    "    vectors = embedder.encode(\n",
    "        prepared,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=normalize_embeddings,\n",
    "    )\n",
    "    return np.asarray(vectors, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Instantiate the embedder and transform all texts into dense vectors.\n",
    "embedder = build_embedder()\n",
    "text_embeddings = embed_texts(embedder, records_df[\"text\"])\n",
    "text_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Build category-level embeddings and cosine-similarity features to enrich the model input space.\n",
    "def build_category_embeddings(\n",
    "    embedder: SentenceTransformer,\n",
    "    category_texts: Iterable[str],\n",
    "    instruction: str = E5_INSTRUCTION,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Generate normalized embeddings for each category description/name.\"\"\"\n",
    "    prepared = [normalize_for_instruction(text, instruction) for text in category_texts]\n",
    "    return embedder.encode(\n",
    "        prepared,\n",
    "        batch_size=len(prepared),\n",
    "        show_progress_bar=False,\n",
    "        normalize_embeddings=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def concat_similarity_features(\n",
    "    text_vectors: np.ndarray,\n",
    "    category_vectors: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Compute cosine similarities and append them to the original embeddings.\"\"\"\n",
    "    similarities = text_vectors @ category_vectors.T\n",
    "    return np.hstack([text_vectors, similarities])\n",
    "\n",
    "\n",
    "category_embeddings = build_category_embeddings(embedder, label_encoder.classes_)\n",
    "augmented_embeddings = concat_similarity_features(text_embeddings, category_embeddings)\n",
    "augmented_embeddings.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Create modeling utilities for CV + holdout evaluation on multiple classifiers, including LightGBM & bagging.\n",
    "def build_model_registry(random_state: int = RANDOM_SEED) -> Dict[str, Callable[[], object]]:\n",
    "    \"\"\"Return a set of lazily-initialized sklearn-compatible estimators with consistent preprocessing.\"\"\"\n",
    "\n",
    "    def make_logistic() -> LogisticRegression:\n",
    "        return LogisticRegression(max_iter=4000, random_state=random_state)\n",
    "\n",
    "    def make_bagging_logistic() -> BaggingClassifier:\n",
    "        return BaggingClassifier(\n",
    "            estimator=make_logistic(),\n",
    "            n_estimators=15,\n",
    "            max_samples=0.85,\n",
    "            bootstrap=True,\n",
    "            random_state=random_state,\n",
    "            n_jobs=None,\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"linear_svm\": lambda: Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"clf\", LinearSVC(random_state=random_state))]\n",
    "        ),\n",
    "        \"logistic_regression\": lambda: Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"clf\", make_logistic())]\n",
    "        ),\n",
    "        \"mlp_classifier\": lambda: Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\n",
    "                    \"clf\",\n",
    "                    MLPClassifier(\n",
    "                        hidden_layer_sizes=(384,),\n",
    "                        activation=\"relu\",\n",
    "                        max_iter=1500,\n",
    "                        random_state=random_state,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        \"lightgbm\": lambda: LGBMClassifier(\n",
    "            n_estimators=600,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.9,\n",
    "            colsample_bytree=0.9,\n",
    "            reg_lambda=0.1,\n",
    "            random_state=random_state,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1,\n",
    "        ),\n",
    "        \"bagging_log_reg\": lambda: Pipeline(\n",
    "            [(\"scaler\", StandardScaler()), (\"clf\", make_bagging_logistic())]\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def cross_validate_models(\n",
    "    model_builders: Dict[str, Callable[[], object]],\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    cv_splits: int = 5,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Run stratified K-fold cross validation and return accuracy/F1 means and stds per model.\"\"\"\n",
    "    splitter = model_selection.StratifiedKFold(\n",
    "        n_splits=cv_splits,\n",
    "        shuffle=True,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    rows: List[Dict[str, float]] = []\n",
    "    for name, builder in model_builders.items():\n",
    "        estimator = builder()\n",
    "        scores = model_selection.cross_validate(\n",
    "            estimator,\n",
    "            features,\n",
    "            labels,\n",
    "            cv=splitter,\n",
    "            scoring=[\"accuracy\", \"f1_macro\"],\n",
    "            n_jobs=None,\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"name\": name,\n",
    "                \"cv_accuracy_mean\": scores[\"test_accuracy\"].mean(),\n",
    "                \"cv_accuracy_std\": scores[\"test_accuracy\"].std(),\n",
    "                \"cv_macro_f1_mean\": scores[\"test_f1_macro\"].mean(),\n",
    "                \"cv_macro_f1_std\": scores[\"test_f1_macro\"].std(),\n",
    "            }\n",
    "        )\n",
    "    return (\n",
    "        pd.DataFrame(rows)\n",
    "        .sort_values(\"cv_macro_f1_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def holdout_report_for_model(\n",
    "    model,\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    label_names: Iterable[str],\n",
    "    test_size: float = 0.2,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> Tuple[Dict[str, float], str]:\n",
    "    \"\"\"Train/validate a single model on a hold-out split and return metrics plus report.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "        features,\n",
    "        labels,\n",
    "        test_size=test_size,\n",
    "        stratify=labels,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    metrics_dict = {\n",
    "        \"accuracy\": metrics.accuracy_score(y_test, preds),\n",
    "        \"macro_f1\": metrics.f1_score(y_test, preds, average=\"macro\"),\n",
    "    }\n",
    "    report = metrics.classification_report(y_test, preds, target_names=list(label_names))\n",
    "    return metrics_dict, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Execute cross validation across all models and inspect their ranking.\n",
    "model_registry = build_model_registry()\n",
    "cv_results = cross_validate_models(model_registry, augmented_embeddings, records_df[\"category_id\"].values)\n",
    "cv_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Evaluate the top-2 CV models (logistic + bagging) and LightGBM on a hold-out split for sanity check.\n",
    "ranked_models = cv_results[\"name\"].tolist()\n",
    "selected = [name for name in ranked_models if name in {\"logistic_regression\", \"bagging_log_reg\", \"lightgbm\"}]\n",
    "reports: List[Dict[str, object]] = []\n",
    "for name in selected:\n",
    "    model = model_registry[name]()\n",
    "    metrics_dict, report = holdout_report_for_model(\n",
    "        model,\n",
    "        augmented_embeddings,\n",
    "        records_df[\"category_id\"].values,\n",
    "        label_encoder.classes_,\n",
    "    )\n",
    "    reports.append({\"name\": name, **metrics_dict, \"report\": report})\n",
    "reports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Use Optuna to tune the Logistic Regression pipeline and push macro-F1 higher via CV.\n",
    "def tune_logistic_with_optuna(\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    n_trials: int = 25,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> optuna.Study:\n",
    "    \"\"\"Optimize logistic regression hyperparameters with Optuna and return the study.\"\"\"\n",
    "\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        C = trial.suggest_float(\"C\", 1e-2, 10.0, log=True)\n",
    "        fit_intercept = trial.suggest_categorical(\"fit_intercept\", [True, False])\n",
    "        class_weight = trial.suggest_categorical(\"class_weight\", [None, \"balanced\"])\n",
    "        tol = trial.suggest_float(\"tol\", 1e-5, 1e-2, log=True)\n",
    "        model = Pipeline(\n",
    "            [\n",
    "                (\"scaler\", StandardScaler()),\n",
    "                (\n",
    "                    \"clf\",\n",
    "                    LogisticRegression(\n",
    "                        C=C,\n",
    "                        fit_intercept=fit_intercept,\n",
    "                        class_weight=class_weight,\n",
    "                        tol=tol,\n",
    "                        max_iter=5000,\n",
    "                        random_state=seed,\n",
    "                    ),\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        splitter = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        scores = model_selection.cross_val_score(\n",
    "            model,\n",
    "            features,\n",
    "            labels,\n",
    "            cv=splitter,\n",
    "            scoring=\"f1_macro\",\n",
    "            n_jobs=None,\n",
    "        )\n",
    "        return float(scores.mean())\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=seed),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    return study\n",
    "\n",
    "\n",
    "log_reg_study = tune_logistic_with_optuna(augmented_embeddings, records_df[\"category_id\"].values)\n",
    "log_reg_study.best_value, log_reg_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Tune LightGBM hyperparameters with Optuna to explore boosted-tree capacity.\n",
    "def tune_lightgbm_with_optuna(\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    n_trials: int = 30,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> optuna.Study:\n",
    "    \"\"\"Optimize LightGBM hyperparameters using Optuna on macro-F1.\"\"\"\n",
    "\n",
    "    def objective(trial: optuna.Trial) -> float:\n",
    "        params = {\n",
    "            \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 64),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2, log=True),\n",
    "            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 30),\n",
    "            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-3, 10.0, log=True),\n",
    "            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-3, 10.0, log=True),\n",
    "        }\n",
    "        model = LGBMClassifier(\n",
    "            n_estimators=600,\n",
    "            subsample=params.pop(\"bagging_fraction\"),\n",
    "            subsample_freq=params.pop(\"bagging_freq\"),\n",
    "            colsample_bytree=params.pop(\"feature_fraction\"),\n",
    "            random_state=seed,\n",
    "            n_jobs=-1,\n",
    "            verbosity=-1,\n",
    "            **params,\n",
    "        )\n",
    "        splitter = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        scores = model_selection.cross_val_score(\n",
    "            model,\n",
    "            features,\n",
    "            labels,\n",
    "            cv=splitter,\n",
    "            scoring=\"f1_macro\",\n",
    "            n_jobs=None,\n",
    "        )\n",
    "        return float(scores.mean())\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        sampler=optuna.samplers.TPESampler(seed=seed),\n",
    "    )\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
    "    return study\n",
    "\n",
    "\n",
    "lightgbm_study = tune_lightgbm_with_optuna(augmented_embeddings, records_df[\"category_id\"].values)\n",
    "lightgbm_study.best_value, lightgbm_study.best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Build stacking/voting ensembles from the Optuna-tuned base models (non-bagging) and evaluate them.\n",
    "def build_tuned_logistic(best_params: Dict[str, object], seed: int = RANDOM_SEED) -> Pipeline:\n",
    "    \"\"\"Instantiate a StandardScaler+LogReg pipeline using Optuna-best hyperparameters.\"\"\"\n",
    "    params = best_params.copy()\n",
    "    return Pipeline(\n",
    "        [\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\n",
    "                \"clf\",\n",
    "                LogisticRegression(\n",
    "                    C=params.get(\"C\", 1.0),\n",
    "                    fit_intercept=params.get(\"fit_intercept\", True),\n",
    "                    class_weight=params.get(\"class_weight\"),\n",
    "                    tol=params.get(\"tol\", 1e-4),\n",
    "                    max_iter=5000,\n",
    "                    random_state=seed,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_tuned_lightgbm(best_params: Dict[str, object], seed: int = RANDOM_SEED) -> LGBMClassifier:\n",
    "    \"\"\"Instantiate a LightGBM classifier with Optuna-best hyperparameters.\"\"\"\n",
    "    params = best_params.copy()\n",
    "    feature_fraction = params.pop(\"feature_fraction\", 0.9)\n",
    "    bagging_fraction = params.pop(\"bagging_fraction\", 0.9)\n",
    "    bagging_freq = params.pop(\"bagging_freq\", 1)\n",
    "    return LGBMClassifier(\n",
    "        n_estimators=600,\n",
    "        subsample=bagging_fraction,\n",
    "        subsample_freq=bagging_freq,\n",
    "        colsample_bytree=feature_fraction,\n",
    "        random_state=seed,\n",
    "        n_jobs=-1,\n",
    "        verbosity=-1,\n",
    "        **params,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_stacking_ensemble(\n",
    "    log_reg: Pipeline,\n",
    "    lgbm: LGBMClassifier,\n",
    "    seed: int = RANDOM_SEED,\n",
    ") -> StackingClassifier:\n",
    "    \"\"\"Create a stacking classifier that blends tuned LogReg and LightGBM.\"\"\"\n",
    "    estimators = [\n",
    "        (\"logreg\", log_reg),\n",
    "        (\"lgbm\", lgbm),\n",
    "    ]\n",
    "    final_estimator = LogisticRegression(max_iter=4000, random_state=seed)\n",
    "    return StackingClassifier(\n",
    "        estimators=estimators,\n",
    "        final_estimator=final_estimator,\n",
    "        stack_method=\"auto\",\n",
    "        passthrough=False,\n",
    "        n_jobs=None,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_voting_ensemble(\n",
    "    log_reg: Pipeline,\n",
    "    lgbm: LGBMClassifier,\n",
    ") -> VotingClassifier:\n",
    "    \"\"\"Return a soft-voting ensemble combining tuned LogReg and LightGBM.\"\"\"\n",
    "    return VotingClassifier(\n",
    "        estimators=[(\"logreg\", log_reg), (\"lgbm\", lgbm)],\n",
    "        voting=\"soft\",\n",
    "        weights=[0.6, 0.4],\n",
    "        n_jobs=None,\n",
    "    )\n",
    "\n",
    "\n",
    "tuned_log_reg = build_tuned_logistic(log_reg_study.best_params)\n",
    "tuned_lgbm = build_tuned_lightgbm(lightgbm_study.best_params)\n",
    "stacking_model = build_stacking_ensemble(tuned_log_reg, tuned_lgbm)\n",
    "voting_model = build_voting_ensemble(tuned_log_reg, tuned_lgbm)\n",
    "stacking_metrics, stacking_report = holdout_report_for_model(\n",
    "    stacking_model,\n",
    "    augmented_embeddings,\n",
    "    records_df[\"category_id\"].values,\n",
    "    label_encoder.classes_,\n",
    ")\n",
    "voting_metrics, voting_report = holdout_report_for_model(\n",
    "    voting_model,\n",
    "    augmented_embeddings,\n",
    "    records_df[\"category_id\"].values,\n",
    "    label_encoder.classes_,\n",
    ")\n",
    "stacking_metrics, voting_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Display the Optuna-ensemble classification reports for qualitative inspection.\n",
    "print(\"=== Stacking ensemble report ===\")\n",
    "print(stacking_report)\n",
    "print(\"=== Voting ensemble report ===\")\n",
    "print(voting_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Fit the tuned logistic model on all data to compute per-sample confidence scores and flag low-confidence predictions.\n",
    "def compute_confidence_table(\n",
    "    model: Pipeline,\n",
    "    features: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    label_names: Iterable[str],\n",
    "    threshold: float = 0.8,\n",
    ") -> Tuple[pd.DataFrame, Pipeline]:\n",
    "    \"\"\"Return a DataFrame with predictions, confidences, and a low-confidence flag.\"\"\"\n",
    "    fitted = model.fit(features, labels)\n",
    "    probs = fitted.predict_proba(features)\n",
    "    pred_ids = probs.argmax(axis=1)\n",
    "    confidence = probs.max(axis=1)\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": records_df[\"id\"],\n",
    "            \"text\": records_df[\"text\"],\n",
    "            \"true_label\": records_df[\"category_label\"],\n",
    "            \"pred_label\": [label_names[idx] for idx in pred_ids],\n",
    "            \"pred_label_id\": pred_ids,\n",
    "            \"confidence\": confidence,\n",
    "        }\n",
    "    )\n",
    "    df[\"low_confidence\"] = df[\"confidence\"] < threshold\n",
    "    return df, fitted\n",
    "\n",
    "\n",
    "confidence_df, tuned_log_reg_fitted = compute_confidence_table(\n",
    "    tuned_log_reg,\n",
    "    augmented_embeddings,\n",
    "    records_df[\"category_id\"].values,\n",
    "    label_encoder.classes_,\n",
    "    threshold=0.85,\n",
    ")\n",
    "confidence_df.sort_values(\"confidence\").head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: List low-confidence cases so reviewers can manually re-label them.\n",
    "low_confidence_cases = confidence_df.query(\"low_confidence\").copy()\n",
    "low_confidence_cases[[\"id\", \"text\", \"pred_label\", \"confidence\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Within each predicted class, surface highly similar texts to batch manual review.\n",
    "def find_similar_within_prediction(\n",
    "    base_embeddings: np.ndarray,\n",
    "    ids: Iterable[int],\n",
    "    texts: Iterable[str],\n",
    "    predicted_labels: Iterable[str],\n",
    "    top_k: int = 3,\n",
    "    min_similarity: float = 0.9,\n",
    ") -> List[Dict[str, object]]:\n",
    "    \"\"\"For each sample, return closest neighbors that share the predicted label.\"\"\"\n",
    "    id_array = np.asarray(list(ids))\n",
    "    text_array = np.asarray(list(texts))\n",
    "    label_array = np.asarray(list(predicted_labels))\n",
    "    cos = cosine_similarity(base_embeddings)\n",
    "    groups: List[Dict[str, object]] = []\n",
    "    for idx in range(len(base_embeddings)):\n",
    "        same_mask = label_array == label_array[idx]\n",
    "        candidate_indices = np.where(same_mask)[0]\n",
    "        sims = cos[idx, candidate_indices]\n",
    "        neighbors = []\n",
    "        for candidate_idx, sim in zip(candidate_indices, sims):\n",
    "            if candidate_idx == idx or sim < min_similarity:\n",
    "                continue\n",
    "            neighbors.append(\n",
    "                {\n",
    "                    \"neighbor_id\": int(id_array[candidate_idx]),\n",
    "                    \"similarity\": float(sim),\n",
    "                    \"neighbor_text\": text_array[candidate_idx],\n",
    "                }\n",
    "            )\n",
    "        if neighbors:\n",
    "            neighbors = sorted(neighbors, key=lambda item: item[\"similarity\"], reverse=True)[:top_k]\n",
    "            groups.append(\n",
    "                {\n",
    "                    \"id\": int(id_array[idx]),\n",
    "                    \"pred_label\": label_array[idx],\n",
    "                    \"text\": text_array[idx],\n",
    "                    \"neighbors\": neighbors,\n",
    "                }\n",
    "            )\n",
    "    return groups\n",
    "\n",
    "\n",
    "similar_groups = find_similar_within_prediction(\n",
    "    text_embeddings,\n",
    "    records_df[\"id\"],\n",
    "    records_df[\"text\"],\n",
    "    confidence_df[\"pred_label\"],\n",
    "    top_k=3,\n",
    "    min_similarity=0.92,\n",
    ")\n",
    "similar_groups[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Purpose: Materialize similar-group output as a DataFrame for downstream tooling.\n",
    "similar_groups_df = pd.json_normalize(similar_groups, sep=\".\")\n",
    "similar_groups_df.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
