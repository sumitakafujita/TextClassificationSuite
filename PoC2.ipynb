{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "907b048b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今日', '言う', '日', '財布', '会社', '忘れる', '仕舞う']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import MeCab\n",
    "import unidic\n",
    "import pathlib\n",
    "\n",
    "dic_path = pathlib.Path(unidic.DICDIR)\n",
    "dic_dir = str(dic_path).replace(\"\\\\\", \"/\")\n",
    "mecab = MeCab.Tagger(f'-r \"{dic_dir}/mecabrc\" -d \"{dic_dir}\"')\n",
    "keep_pos = {\"名詞\", \"動詞\", \"形容詞\", \"副詞\"}\n",
    "\n",
    "def mecab_tokenizer(text: str):\n",
    "    text = str(text).replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "    node = mecab.parseToNode(text)\n",
    "    tokens = []\n",
    "\n",
    "    while node:\n",
    "        surface = node.surface\n",
    "        # BOS/EOS や空文字はスキップ\n",
    "        if not surface:\n",
    "            node = node.next\n",
    "            continue\n",
    "\n",
    "        features = node.feature.split(\",\")\n",
    "        pos = features[0]      # 品詞 大分類（名詞, 動詞, 助詞, など）\n",
    "\n",
    "        # ★ 名詞・動詞・形容詞・副詞 以外は捨てる\n",
    "        if pos not in keep_pos:\n",
    "            node = node.next\n",
    "            continue\n",
    "\n",
    "        # ==== 原形（lemma）を優先して使う ====\n",
    "        # UniDic 系の典型的な並び:\n",
    "        #  0: pos1, 1: pos2, 2: pos3, 3: pos4,\n",
    "        #  4: cType, 5: cForm, 6: lForm, 7: lemma, 8: orth, 9: pron, ...\n",
    "        lemma_idx = 7\n",
    "        orth_idx  = 8\n",
    "\n",
    "        token = surface  # デフォルトは表層形\n",
    "\n",
    "        if len(features) > lemma_idx and features[lemma_idx] not in (\"*\", \"\"):\n",
    "            token = features[lemma_idx]   # 漢字混じりの原形\n",
    "        elif len(features) > orth_idx and features[orth_idx] not in (\"*\", \"\"):\n",
    "            token = features[orth_idx]    # 文中表記\n",
    "\n",
    "        tokens.append(token)\n",
    "        node = node.next\n",
    "\n",
    "    return tokens\n",
    "\n",
    "mecab_tokenizer(\"ああ、今日はなんていう日だ。お財布を会社に忘れてしまうなんて。。。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad180bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "############################################\n",
    "# 2. ユーティリティ\n",
    "############################################\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    norm = np.linalg.norm(x, axis=-1, keepdims=True) + 1e-12\n",
    "    return x / norm\n",
    "\n",
    "def log_softmax(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x_max = np.max(x)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(x - x_max))) + x_max\n",
    "    return x - log_sum_exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdc874a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "############################################\n",
    "# 3. vMF 風のロバスト分類器\n",
    "############################################\n",
    "\n",
    "class RobustTextClassifier:\n",
    "    \"\"\"\n",
    "    - 各カテゴリ k について、トークン埋め込みの平均方向 μ_k, 簡易集中度 κ_k を学習\n",
    "      （vMF 分布の log p(x|k) ∝ κ_k * μ_k^T x を模している）\n",
    "    - 社内語（inhouse_terms）は埋め込みには使わず、p(k|w) を別で推定して加点\n",
    "    \"\"\"\n",
    "    dic_path = pathlib.Path(unidic.DICDIR)\n",
    "    dic_dir = str(dic_path).replace(\"\\\\\", \"/\")\n",
    "    mecab = MeCab.Tagger(f'-r \"{dic_dir}/mecabrc\" -d \"{dic_dir}\"')\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        categories,\n",
    "        st_model=\"stsb-xlm-r-multilingual\",\n",
    "        keep_pos: set[str] = {\"名詞\", \"動詞\", \"形容詞\", \"副詞\"},\n",
    "        inhouse_terms = None,\n",
    "        alpha_inhouse=1.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        categories: カテゴリ名のリスト\n",
    "        st_model: SentenceTransformer インスタンス\n",
    "        tokenizer: str -> List[str] のトークン化関数\n",
    "        inhouse_terms: 社内語（固有名詞など）の set\n",
    "        alpha_inhouse: p(k|w) をどれくらい効かせるか\n",
    "        \"\"\"\n",
    "        self.categories = list(categories)\n",
    "        self.num_classes = len(categories)\n",
    "        self.cat2idx = {c: i for i, c in enumerate(self.categories)}\n",
    "\n",
    "        self.keep_pos = keep_pos\n",
    "        self.st_model = SentenceTransformer(st_model)\n",
    "        self.inhouse_terms = set(inhouse_terms or [])\n",
    "        self.alpha_inhouse = alpha_inhouse\n",
    "\n",
    "        # パラメータ（fit でセット）\n",
    "        self.mu = None              # (K, D) クラスごとの平均方向\n",
    "        self.kappa = None           # (K,) 簡易集中度\n",
    "        self.class_priors = None    # (K,) p(k)\n",
    "        self.inhouse_p_kw = {}      # dict[w] -> np.array shape (K,)\n",
    "\n",
    "\n",
    "    def _tokenizer(self, text: str):\n",
    "        text = str(text).replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "        node = self.mecab.parseToNode(text)\n",
    "        tokens = []\n",
    "\n",
    "        while node:\n",
    "            surface = node.surface\n",
    "            # BOS/EOS や空文字はスキップ\n",
    "            if not surface:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            features = node.feature.split(\",\")\n",
    "            pos = features[0]      # 品詞 大分類（名詞, 動詞, 助詞, など）\n",
    "\n",
    "            # ★ 名詞・動詞・形容詞・副詞 以外は捨てる\n",
    "            if pos not in self.keep_pos:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            # ==== 原形（lemma）を優先して使う ====\n",
    "            # UniDic 系の典型的な並び:\n",
    "            #  0: pos1, 1: pos2, 2: pos3, 3: pos4,\n",
    "            #  4: cType, 5: cForm, 6: lForm, 7: lemma, 8: orth, 9: pron, ...\n",
    "            lemma_idx = 7\n",
    "            orth_idx  = 8\n",
    "\n",
    "            token = surface  # デフォルトは表層形\n",
    "\n",
    "            if len(features) > lemma_idx and features[lemma_idx] not in (\"*\", \"\"):\n",
    "                token = features[lemma_idx]   # 漢字混じりの原形\n",
    "            elif len(features) > orth_idx and features[orth_idx] not in (\"*\", \"\"):\n",
    "                token = features[orth_idx]    # 文中表記\n",
    "\n",
    "            tokens.append(token)\n",
    "            node = node.next\n",
    "\n",
    "        return tokens\n",
    "    \n",
    "    ########################################\n",
    "    # 内部: トークン埋め込み\n",
    "    ########################################\n",
    "    def _embed_tokens(self, tokens):\n",
    "        if not tokens:\n",
    "            D = self.st_model.get_sentence_embedding_dimension()\n",
    "            return np.zeros((0, D))\n",
    "        vecs = self.st_model.encode(\n",
    "            tokens,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,  # L2 正規化済み\n",
    "        )\n",
    "        return vecs\n",
    "\n",
    "    ########################################\n",
    "    # 学習\n",
    "    ########################################\n",
    "    def fit(self, docs, labels):\n",
    "        \"\"\"\n",
    "        docs: 文書のリスト（文字列）\n",
    "        labels: 各文書のカテゴリ名\n",
    "        \"\"\"\n",
    "        label_idx = np.array([self.cat2idx[y] for y in labels])\n",
    "        class_counts = np.bincount(label_idx, minlength=self.num_classes)\n",
    "        self.class_priors = class_counts / class_counts.sum()\n",
    "\n",
    "        D = self.st_model.get_sentence_embedding_dimension()\n",
    "        class_vec_sums = np.zeros((self.num_classes, D), dtype=np.float64)\n",
    "        class_vec_counts = np.zeros(self.num_classes, dtype=np.float64)\n",
    "\n",
    "        # 社内語のカウント（p(k|w) 学習用）\n",
    "        # inhouse_counts = defaultdict(lambda: np.zeros(self.num_classes, dtype=np.float64))\n",
    "\n",
    "        for doc, y in zip(docs, labels):\n",
    "            k = self.cat2idx[y]\n",
    "            tokens = self._tokenizer(doc)\n",
    "\n",
    "            normal_tokens = [t for t in tokens if t not in self.inhouse_terms]\n",
    "            # inhouse_tokens = [t for t in tokens if t in self.inhouse_terms]\n",
    "\n",
    "            # 社内語の出現カウント\n",
    "            # for w in inhouse_tokens:\n",
    "            #     inhouse_counts[w][k] += 1.0\n",
    "\n",
    "            if not normal_tokens:\n",
    "                continue\n",
    "\n",
    "            vecs = self._embed_tokens(normal_tokens)\n",
    "            class_vec_sums[k] += vecs.sum(axis=0)\n",
    "            class_vec_counts[k] += vecs.shape[0]\n",
    "\n",
    "        # クラスごとの平均方向 μ_k, 簡易 κ_k を推定\n",
    "        self.mu = np.zeros((self.num_classes, D), dtype=np.float64)\n",
    "        self.kappa = np.zeros(self.num_classes, dtype=np.float64)\n",
    "\n",
    "        for k in range(self.num_classes):\n",
    "            if class_vec_counts[k] == 0:\n",
    "                continue\n",
    "            mean_vec = class_vec_sums[k] / (class_vec_counts[k] + 1e-12)\n",
    "            self.mu[k] = l2_normalize(mean_vec.reshape(1, -1))[0]\n",
    "            R = np.linalg.norm(mean_vec)   # 平均ベクトル長 ≒ 平均コサイン類似度\n",
    "            self.kappa[k] = 10.0 * R      # ざっくりしたスケーリング（チューニング可）\n",
    "\n",
    "        # 社内語ごとの p(k|w) を推定\n",
    "        # for w, counts in inhouse_counts.items():\n",
    "        #     if counts.sum() == 0:\n",
    "        #         continue\n",
    "        #     probs = counts / counts.sum()\n",
    "        #     self.inhouse_p_kw[w] = probs   # shape (K,)\n",
    "\n",
    "    ########################################\n",
    "    # 単語レベルの log p(x|k) （vMF 風）\n",
    "    ########################################\n",
    "    def _log_p_x_given_k(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (D,)\n",
    "        戻り値: shape (K,) 各クラスの「vMF っぽい」対数スコア（正規化定数は無視）\n",
    "        log p(x|k) ∝ κ_k * μ_k^T x\n",
    "        \"\"\"\n",
    "        cos_sim = self.mu @ x  # (K,D) · (D,) -> (K,)\n",
    "        return self.kappa * cos_sim\n",
    "\n",
    "    ########################################\n",
    "    # 文書のスコアリング\n",
    "    ########################################\n",
    "    def score_document(self, doc: str):\n",
    "        tokens = self._tokenizer(doc)\n",
    "        normal_tokens = tokens #= [t for t in tokens if t not in self.inhouse_terms]\n",
    "        # inhouse_tokens = [t for t in tokens if t in self.inhouse_terms]\n",
    "\n",
    "        log_likelihood = np.zeros(self.num_classes, dtype=np.float64)\n",
    "\n",
    "        # 1. 埋め込みベース（通常トークンのみ）\n",
    "        if normal_tokens:\n",
    "            vecs = self._embed_tokens(normal_tokens)\n",
    "            for x in vecs:\n",
    "                x = l2_normalize(x.reshape(1, -1))[0]\n",
    "                log_likelihood += self._log_p_x_given_k(x)\n",
    "\n",
    "        # 2. 社内語由来のバイアス α * Σ log p(k|w)\n",
    "        # if self.alpha_inhouse > 0 and inhouse_tokens:\n",
    "        #     for w in inhouse_tokens:\n",
    "        #         if w not in self.inhouse_p_kw:\n",
    "        #             continue\n",
    "        #         log_p_kw = np.log(self.inhouse_p_kw[w] + 1e-12)\n",
    "        #         log_likelihood += self.alpha_inhouse * log_p_kw\n",
    "\n",
    "        # 3. クラス事前確率 log p(k) を加える\n",
    "        log_prior = np.log(self.class_priors + 1e-12)\n",
    "        log_post_unnorm = log_prior + log_likelihood\n",
    "        return log_post_unnorm\n",
    "\n",
    "    def predict_proba(self, doc: str):\n",
    "        log_scores = self.score_document(doc)\n",
    "        log_probs = log_softmax(log_scores)\n",
    "        return np.exp(log_probs)\n",
    "\n",
    "    def predict(self, doc: str):\n",
    "        probs = self.predict_proba(doc)\n",
    "        k = int(np.argmax(probs))\n",
    "        return self.categories[k]\n",
    "\n",
    "\n",
    "#     # 7. 評価\n",
    "#     print(\"Evaluating...\")\n",
    "#     y_pred = [clf.predict(doc) for doc in X_test]\n",
    "#     acc = accuracy_score(y_test, y_pred)\n",
    "#     print(\"Accuracy:\", acc)\n",
    "#     print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "#     # 8. サンプル文で試す\n",
    "#     sample = \"ホワイトボードに在席状況を入力して、他部署からも確認できるようにしたい。\"\n",
    "#     print(\"sample:\", sample)\n",
    "#     print(\"tokens:\", mecab_tokenizer(sample))\n",
    "#     print(\"pred:\", clf.predict(sample))\n",
    "#     print(\"proba:\", clf.predict_proba(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da1feae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pathlib\n",
    "import MeCab\n",
    "import unidic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "############################################\n",
    "# ユーティリティ\n",
    "############################################\n",
    "\n",
    "def l2_normalize(x: np.ndarray) -> np.ndarray:\n",
    "    norm = np.linalg.norm(x, axis=-1, keepdims=True) + 1e-12\n",
    "    return x / norm\n",
    "\n",
    "def log_softmax(x):\n",
    "    x = np.asarray(x, dtype=np.float64)\n",
    "    x_max = np.max(x)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(x - x_max))) + x_max\n",
    "    return x - log_sum_exp\n",
    "\n",
    "\n",
    "############################################\n",
    "# vMF 風のロバスト分類器（trimmed mean + λ(x) + 文書距離）\n",
    "############################################\n",
    "\n",
    "class RobustTextClassifier:\n",
    "    \"\"\"\n",
    "    - 各カテゴリ k について、トークン埋め込みの代表方向 μ_k, 簡易集中度 κ_k を学習\n",
    "      （vMF 分布の log p(x|k) ∝ κ_k * μ_k^T x を模している）\n",
    "    - μ_k は「trimmed mean」（外れ方向を落とした平均）で推定\n",
    "    - 推論時は、トークンごとに λ(x) を計算し、\n",
    "      「どのクラスから見ても遠いベクトル」は log p への寄与を弱める\n",
    "    - さらに、文書ベクトルの計算と文書間距離の計算機能を提供\n",
    "    \"\"\"\n",
    "\n",
    "    # MeCab をクラス変数として 1 個だけ持つ\n",
    "    dic_path = pathlib.Path(unidic.DICDIR)\n",
    "    dic_dir = str(dic_path).replace(\"\\\\\", \"/\")\n",
    "    mecab = MeCab.Tagger(f'-r \"{dic_dir}/mecabrc\" -d \"{dic_dir}\"')\n",
    "    mecab.parse(\"\")  # GC 対策\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        categories,\n",
    "        st_model: str = \"stsb-xlm-r-multilingual\",\n",
    "        keep_pos: set[str] = {\"名詞\", \"動詞\", \"形容詞\", \"副詞\"},\n",
    "        inhouse_terms=None,\n",
    "        alpha_inhouse: float = 1.0,\n",
    "        trim_ratio: float = 0.2,        # trimmed mean で落とす割合（低類似度側）\n",
    "        lambda_low: float = 0.0,        # λ(x) が 0 になる cos 類似度の下限\n",
    "        lambda_high: float = 0.8,       # λ(x) が 1 になる cos 類似度の上限\n",
    "    ):\n",
    "        \"\"\"\n",
    "        categories: カテゴリ名のリスト\n",
    "        st_model: SentenceTransformer のモデル名\n",
    "        keep_pos: 残す品詞の集合\n",
    "        inhouse_terms: 社内語（固有名詞など）の set（いまは未使用でもOK）\n",
    "        alpha_inhouse: 将来 p(k|w) を効かせるときの重み\n",
    "        trim_ratio: 類似度の低い側から何割を trimmed mean で捨てるか\n",
    "        lambda_low, lambda_high: λ(x) の線形スケール範囲\n",
    "        \"\"\"\n",
    "        self.categories = list(categories)\n",
    "        self.num_classes = len(categories)\n",
    "        self.cat2idx = {c: i for i, c in enumerate(self.categories)}\n",
    "\n",
    "        self.keep_pos = keep_pos\n",
    "        self.st_model = SentenceTransformer(st_model)\n",
    "        self.inhouse_terms = set(inhouse_terms or [])\n",
    "        self.alpha_inhouse = alpha_inhouse\n",
    "\n",
    "        self.trim_ratio = trim_ratio\n",
    "        self.lambda_low = lambda_low\n",
    "        self.lambda_high = lambda_high\n",
    "\n",
    "        # パラメータ（fit でセット）\n",
    "        self.mu = None              # (K, D) クラスごとの代表方向\n",
    "        self.kappa = None           # (K,) 簡易集中度\n",
    "        self.class_priors = None    # (K,) p(k)\n",
    "        self.inhouse_p_kw = {}      # dict[w] -> np.array shape (K,)\n",
    "\n",
    "    ########################################\n",
    "    # tokenizer（MeCab + 品詞フィルタ + lemma）\n",
    "    ########################################\n",
    "    def _tokenizer(self, text: str):\n",
    "        text = str(text).replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "\n",
    "        node = self.mecab.parseToNode(text)\n",
    "        tokens = []\n",
    "\n",
    "        while node:\n",
    "            surface = node.surface\n",
    "            # BOS/EOS や空文字はスキップ\n",
    "            if not surface:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            features = node.feature.split(\",\")\n",
    "            pos = features[0]      # 品詞 大分類（名詞, 動詞, 助詞, など）\n",
    "\n",
    "            # 名詞・動詞・形容詞・副詞 以外は捨てる\n",
    "            if pos not in self.keep_pos:\n",
    "                node = node.next\n",
    "                continue\n",
    "\n",
    "            # UniDic 系の典型的な並び:\n",
    "            #  0: pos1, 1: pos2, 2: pos3, 3: pos4,\n",
    "            #  4: cType, 5: cForm, 6: lForm, 7: lemma, 8: orth, 9: pron, ...\n",
    "            lemma_idx = 7\n",
    "            orth_idx  = 8\n",
    "\n",
    "            token = surface  # デフォルトは表層形\n",
    "\n",
    "            if len(features) > lemma_idx and features[lemma_idx] not in (\"*\", \"\"):\n",
    "                token = features[lemma_idx]   # 漢字混じりの原形\n",
    "            elif len(features) > orth_idx and features[orth_idx] not in (\"*\", \"\"):\n",
    "                token = features[orth_idx]    # 文中表記\n",
    "\n",
    "            tokens.append(token)\n",
    "            node = node.next\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    ########################################\n",
    "    # 内部: トークン埋め込み\n",
    "    ########################################\n",
    "    def _embed_tokens(self, tokens):\n",
    "        if not tokens:\n",
    "            D = self.st_model.get_sentence_embedding_dimension()\n",
    "            return np.zeros((0, D))\n",
    "        vecs = self.st_model.encode(\n",
    "            tokens,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True,  # L2 正規化済み\n",
    "        )\n",
    "        return vecs\n",
    "\n",
    "    ########################################\n",
    "    # 内部: λ(x)（トークンごとの重み）を計算\n",
    "    ########################################\n",
    "    def _lambda_for_token(self, x: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        x: shape (D,), L2 正規化済み前提\n",
    "        戻り値: 0.0〜1.0 の重み（どのクラスからも遠いトークンほど 0 に近づく）\n",
    "        \"\"\"\n",
    "        if self.mu is None:\n",
    "            return 1.0  # まだ学習前なら無条件に重み1\n",
    "\n",
    "        cos_all = self.mu @ x            # shape: (K,)\n",
    "        cos_max = float(np.max(cos_all)) # どのクラス方向とも似ていなければ小さい\n",
    "\n",
    "        if self.lambda_high > self.lambda_low:\n",
    "            lam = (cos_max - self.lambda_low) / (self.lambda_high - self.lambda_low)\n",
    "            lam = float(np.clip(lam, 0.0, 1.0))\n",
    "        else:\n",
    "            lam = 1.0\n",
    "\n",
    "        return lam\n",
    "\n",
    "    ########################################\n",
    "    # 学習（trimmed mean で μ_k を推定）\n",
    "    ########################################\n",
    "    def fit(self, docs, labels):\n",
    "        \"\"\"\n",
    "        docs: 文書のリスト（文字列）\n",
    "        labels: 各文書のカテゴリ名\n",
    "        \"\"\"\n",
    "        label_idx = np.array([self.cat2idx[y] for y in labels])\n",
    "        class_counts = np.bincount(label_idx, minlength=self.num_classes)\n",
    "        self.class_priors = class_counts / class_counts.sum()\n",
    "\n",
    "        D = self.st_model.get_sentence_embedding_dimension()\n",
    "\n",
    "        # 各クラスごとのトークン埋め込みを全部ためる\n",
    "        per_class_vecs = [[] for _ in range(self.num_classes)]\n",
    "\n",
    "        for doc, y in zip(docs, labels):\n",
    "            k = self.cat2idx[y]\n",
    "            tokens = self._tokenizer(doc)\n",
    "            normal_tokens = tokens\n",
    "\n",
    "            if not normal_tokens:\n",
    "                continue\n",
    "\n",
    "            vecs = self._embed_tokens(normal_tokens)\n",
    "            for v in vecs:\n",
    "                per_class_vecs[k].append(v)\n",
    "\n",
    "        # クラスごとの代表方向 μ_k と κ_k を trimmed mean で推定\n",
    "        self.mu = np.zeros((self.num_classes, D), dtype=np.float64)\n",
    "        self.kappa = np.zeros(self.num_classes, dtype=np.float64)\n",
    "\n",
    "        for k in range(self.num_classes):\n",
    "            if not per_class_vecs[k]:\n",
    "                continue\n",
    "\n",
    "            X = np.stack(per_class_vecs[k], axis=0)  # shape: (Nk, D)\n",
    "\n",
    "            # 1) まず普通の平均で初期方向を作る\n",
    "            mean0 = l2_normalize(X.mean(axis=0, keepdims=True))[0]\n",
    "\n",
    "            # 2) 各ベクトルとの cos 類似度を計算\n",
    "            cos = X @ mean0  # shape: (Nk,)\n",
    "\n",
    "            # 3) 類似度の低い方から trim_ratio を落とす\n",
    "            if 0.0 < self.trim_ratio < 0.5:\n",
    "                thresh = np.quantile(cos, self.trim_ratio)\n",
    "                mask = cos >= thresh\n",
    "                X_trim = X[mask]\n",
    "            else:\n",
    "                X_trim = X\n",
    "\n",
    "            # 4) trimmed mean を代表方向とする\n",
    "            mean_vec = X_trim.mean(axis=0)\n",
    "            self.mu[k] = l2_normalize(mean_vec.reshape(1, -1))[0]\n",
    "\n",
    "            # 5) 平均ベクトル長 R から κ をざっくり決める\n",
    "            R = np.linalg.norm(mean_vec)\n",
    "            self.kappa[k] = 10.0 * R  # スケーリングはチューニングポイント\n",
    "\n",
    "    ########################################\n",
    "    # 単語レベルの log p(x|k) （vMF 風）\n",
    "    ########################################\n",
    "    def _log_p_x_given_k(self, x):\n",
    "        \"\"\"\n",
    "        x: shape (D,)\n",
    "        戻り値: shape (K,) 各クラスの「vMF っぽい」対数スコア（正規化定数は無視）\n",
    "        log p(x|k) ∝ κ_k * μ_k^T x\n",
    "        \"\"\"\n",
    "        cos_sim = self.mu @ x  # (K,D) · (D,) -> (K,)\n",
    "        return self.kappa * cos_sim\n",
    "\n",
    "    ########################################\n",
    "    # 文書のスコアリング（λ(x) でノイズトークンを弱める）\n",
    "    ########################################\n",
    "    def score_document(self, doc: str):\n",
    "        tokens = self._tokenizer(doc)\n",
    "        normal_tokens = tokens\n",
    "\n",
    "        log_likelihood = np.zeros(self.num_classes, dtype=np.float64)\n",
    "\n",
    "        # 1. 埋め込みベース（トークンごと）\n",
    "        if normal_tokens:\n",
    "            vecs = self._embed_tokens(normal_tokens)\n",
    "            for x in vecs:\n",
    "                x = l2_normalize(x.reshape(1, -1))[0]\n",
    "                lam = self._lambda_for_token(x)\n",
    "                if lam <= 0.0:\n",
    "                    continue  # 完全にノイズ扱い\n",
    "                log_likelihood += lam * self._log_p_x_given_k(x)\n",
    "\n",
    "        # 2. クラス事前確率 log p(k) を加える\n",
    "        log_prior = np.log(self.class_priors + 1e-12)\n",
    "        log_post_unnorm = log_prior + log_likelihood\n",
    "        return log_post_unnorm\n",
    "\n",
    "    def predict_proba(self, doc: str):\n",
    "        log_scores = self.score_document(doc)\n",
    "        log_probs = log_softmax(log_scores)\n",
    "        return np.exp(log_probs)\n",
    "\n",
    "    def predict(self, doc: str):\n",
    "        probs = self.predict_proba(doc)\n",
    "        k = int(np.argmax(probs))\n",
    "        return self.categories[k]\n",
    "\n",
    "    ########################################\n",
    "    # 文書ベクトル（1つのベクトル）を計算\n",
    "    ########################################\n",
    "    def embed_document(self, doc: str, use_lambda_weight: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        文書全体を 1 つのベクトルにまとめる。\n",
    "        - トークン埋め込みの（重み付き）平均\n",
    "        - use_lambda_weight=True のとき、\n",
    "          λ(x) を重みとして「どのクラスからも遠いトークン」を自動的に弱める\n",
    "        \"\"\"\n",
    "        tokens = self._tokenizer(doc)\n",
    "        if not tokens:\n",
    "            D = self.st_model.get_sentence_embedding_dimension()\n",
    "            return np.zeros(D, dtype=np.float64)\n",
    "\n",
    "        vecs = self._embed_tokens(tokens)  # shape: (N, D)\n",
    "        if vecs.shape[0] == 0:\n",
    "            D = self.st_model.get_sentence_embedding_dimension()\n",
    "            return np.zeros(D, dtype=np.float64)\n",
    "\n",
    "        # トークンごとの重み（デフォルト 1）\n",
    "        weights = np.ones(vecs.shape[0], dtype=np.float64)\n",
    "\n",
    "        if use_lambda_weight and self.mu is not None:\n",
    "            for i in range(vecs.shape[0]):\n",
    "                x = l2_normalize(vecs[i].reshape(1, -1))[0]\n",
    "                weights[i] = self._lambda_for_token(x)\n",
    "\n",
    "        # 全部 0 になってしまった場合は等重み平均にフォールバック\n",
    "        if np.all(weights <= 0):\n",
    "            weights = np.ones_like(weights)\n",
    "\n",
    "        weights = weights / (weights.sum() + 1e-12)\n",
    "        doc_vec = (weights[:, None] * vecs).sum(axis=0)\n",
    "        doc_vec = l2_normalize(doc_vec.reshape(1, -1))[0]\n",
    "        return doc_vec\n",
    "\n",
    "    ########################################\n",
    "    # 文書間の距離を計算\n",
    "    ########################################\n",
    "    def document_distance(\n",
    "        self,\n",
    "        doc1: str,\n",
    "        doc2: str,\n",
    "        metric: str = \"cosine\",\n",
    "        use_lambda_weight: bool = True,\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        2つの文書 doc1, doc2 の距離を計算する。\n",
    "        - まず embed_document で文書ベクトルを計算\n",
    "        - metric:\n",
    "          - \"cosine\": 1 - コサイン類似度\n",
    "          - \"euclidean\": ユークリッド距離\n",
    "        \"\"\"\n",
    "        v1 = self.embed_document(doc1, use_lambda_weight=use_lambda_weight)\n",
    "        v2 = self.embed_document(doc2, use_lambda_weight=use_lambda_weight)\n",
    "\n",
    "        if metric == \"cosine\":\n",
    "            # embed_document は L2 正規化しているので dot のみでOK\n",
    "            sim = float(np.dot(v1, v2))\n",
    "            sim = max(min(sim, 1.0), -1.0)\n",
    "            return 1.0 - sim\n",
    "        elif metric == \"euclidean\":\n",
    "            return float(np.linalg.norm(v1 - v2))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "698aabfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Accuracy: 0.16279069767441862\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ESG      0.000     0.000     0.000        20\n",
      "        サービス      0.000     0.000     0.000        22\n",
      "        商品開発      0.000     0.000     0.000        13\n",
      "          営業      0.000     0.000     0.000        26\n",
      "        新規事業      0.000     0.000     0.000        14\n",
      "          施工      0.237     0.250     0.243        36\n",
      "          生産      0.000     0.000     0.000         5\n",
      "     福利厚生・制度      0.000     0.000     0.000        59\n",
      "          総務      0.151     0.868     0.258        38\n",
      "          設計      0.000     0.000     0.000        25\n",
      "\n",
      "    accuracy                          0.163       258\n",
      "   macro avg      0.039     0.112     0.050       258\n",
      "weighted avg      0.055     0.163     0.072       258\n",
      "\n",
      "sample: ホワイトボードに在席状況を入力して、他部署からも確認できるようにしたい。\n",
      "tokens: ['ホワイトボード-whiteboard', '在席', '状況', '入力', '為る', '他', '部署', '確認', '出来る', '為る']\n",
      "pred: 総務\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fujita096\\PJ\\TextClassificationSuite\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\fujita096\\PJ\\TextClassificationSuite\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "c:\\Users\\fujita096\\PJ\\TextClassificationSuite\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proba: [0.02280771 0.01217608 0.00437027 0.10982883 0.00312859 0.19398225\n",
      " 0.01735945 0.23393069 0.34333011 0.05908601]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# 1. データ読み込み\n",
    "df = pd.read_csv(\"data/data.csv\")\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "labels = df[\"category\"].astype(str).tolist()\n",
    "\n",
    "# 2. train / test に分割\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "rlf = RobustTextClassifier(\n",
    "    categories=sorted(set(labels)),  # 社内語の p(k|w) をどれくらい効かせるか\n",
    ")\n",
    "\n",
    "rlf.fit(X_train, y_train)\n",
    "\n",
    "print(\"Evaluating...\")\n",
    "y_pred = [rlf.predict(doc) for doc in X_test]\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(classification_report(y_test, y_pred, digits=3))\n",
    "\n",
    "# 8. サンプル文で試す\n",
    "sample = \"ホワイトボードに在席状況を入力して、他部署からも確認できるようにしたい。\"\n",
    "print(\"sample:\", sample)\n",
    "print(\"tokens:\", mecab_tokenizer(sample))\n",
    "print(\"pred:\", rlf.predict(sample))\n",
    "print(\"proba:\", rlf.predict_proba(sample))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8e53846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地域に根差す産婆（助産院や自宅出産を扱う開業助産師を指す）は妊娠中から密に妊婦とその家族に寄り添う健診をして出産はもちろん産後やその後も相談役となり、産後うつや虐待の予防となる。 助産院は集いの場となり母親は子育ての仲間ができ安心した妊娠、出産から子育てと続き少子化対策や地域活性化につながる。 災害時の支援力も注目された。 そんな命を守り続ける存在の周知活動として無形文化遺産登録に向け活動中。\n",
      "ESG: 0.0001224798842212269\n",
      "サービス: 1.8183392354298897e-05\n",
      "商品開発: 3.517567153490362e-07\n",
      "営業: 0.040364919021515726\n",
      "新規事業: 4.322642061452146e-07\n",
      "施工: 0.32417260253473934\n",
      "生産: 0.0040560613687497075\n",
      "福利厚生・制度: 0.06681398772255664\n",
      "総務: 0.5621810463378332\n",
      "設計: 0.0022699357171027694\n",
      "---\n",
      "建売の計画をコンペ形式で実施したいです。\n",
      "設計をされている方は学生時代に設計アイデアコンペに応募した方も多いのではないでしょうか。\n",
      "全国から集まった選りすぐりのプランの建売コンペはやりがい、見ごたえ共に十分かと思います。\n",
      "実例を評価するコンペはありますが、アイデア・構想の設計コンペはあまりないのではと思います。\n",
      "また、本アイデアが入賞した際の賞金を建売コンペ1位の方への賞金としたいと考えています。\n",
      "ESG: 0.0003191921001668419\n",
      "サービス: 3.30493555084513e-05\n",
      "商品開発: 3.1883868283791865e-06\n",
      "営業: 0.03931154975963855\n",
      "新規事業: 1.3202049949196588e-06\n",
      "施工: 0.3167025584081201\n",
      "生産: 0.010958343425647844\n",
      "福利厚生・制度: 0.07345535278677465\n",
      "総務: 0.5353667366985726\n",
      "設計: 0.023848708873742217\n",
      "---\n",
      "クリエイティブな業務を行う設計やコーディネーターは、良いものを作るためにはやる気・モチベーションが大切です。モチベーションアップのために、誰かから誰かへの良かった事を報告してもらいます、つまり「ラブレター」です。\n",
      "小さな出来事や立ち振る舞いなど、褒める対象は広く、若手も平等に評価されるように評価のハードルも低く設定します。\n",
      "ESG: 0.0014388984970877085\n",
      "サービス: 0.0002393779232256903\n",
      "商品開発: 4.098504894854571e-05\n",
      "営業: 0.05259710721737573\n",
      "新規事業: 1.4784449923062737e-05\n",
      "施工: 0.2732961759383504\n",
      "生産: 0.010490691036139591\n",
      "福利厚生・制度: 0.1058988973866427\n",
      "総務: 0.537861810179044\n",
      "設計: 0.018121272323264843\n",
      "---\n",
      "就職活動時に働き方や福利厚生、職場環境のことがメインになってきていますが、積水ハウスの設計士はこんな物件を作っているんだ！という就活生に向けて設計士が自分の作品をコンペの発表会のようにプレゼンで伝える就職活動サポートです。\n",
      "設計士の考え方やハウスメーカーの考え方も合わせて学生が習得できます。就活時の印象はいつまでも残りいつまでも前向きに仕事ができる社員を増やしていこうという目的です。\n",
      "ESG: 0.0001453960902424071\n",
      "サービス: 1.5738550890965285e-05\n",
      "商品開発: 5.960474349143319e-07\n",
      "営業: 0.027347238823100434\n",
      "新規事業: 9.952634944713787e-07\n",
      "施工: 0.48380252274950364\n",
      "生産: 0.0063268106984998105\n",
      "福利厚生・制度: 0.057749736878938286\n",
      "総務: 0.41746357214862373\n",
      "設計: 0.007147392749281469\n",
      "---\n",
      "コンカー申請の簡略化を提案させて頂きます。　コンカーの立替え申請を行いますが、外部システムのため、申請システムがとにかく難しいです。　今後、代理申請にて総務課に領収書を提出して、簡単に処理ができるようにして欲しい。　工事責任者も多忙なこと。また、積水ハウス建設の現業社員方にも簡単に処理ができる必要があります。　そのため、コンカー申請の代理申請を提案させて頂きます。\n",
      "\n",
      "ESG: 0.0001188183560151134\n",
      "サービス: 1.8963917095618764e-05\n",
      "商品開発: 1.1994458490516115e-06\n",
      "営業: 0.022964545343065072\n",
      "新規事業: 6.234388764750247e-07\n",
      "施工: 0.41452820443201804\n",
      "生産: 0.009168594470790013\n",
      "福利厚生・制度: 0.0341631280987014\n",
      "総務: 0.5065228278900523\n",
      "設計: 0.012513094607528289\n",
      "---\n",
      "皆様の家のお掃除ロボットはどこにありますか？\n",
      "お掃除ロボットが通れるように下の開いた収納建具が工場出荷材になれば需要はたくさんあるはず！\n",
      "お掃除ロボットの基地を隠して見た目もすっきりです。\n",
      "いいね！ご意見お待ちしてます!!\n",
      "ESG: 0.0026000321548528163\n",
      "サービス: 0.000644068846171319\n",
      "商品開発: 0.00013346529025559038\n",
      "営業: 0.07456977689343076\n",
      "新規事業: 5.941778794781717e-05\n",
      "施工: 0.2628900627813198\n",
      "生産: 0.015086834841918365\n",
      "福利厚生・制度: 0.11875539603432542\n",
      "総務: 0.5094178140410356\n",
      "設計: 0.015843131328738425\n",
      "---\n",
      "全社の流れである「人財還流」。新天地でのスタートにあたり、快適な単身赴任住居の確保と選択は重要かつ負担の大きいミッションです。積水ハウスグループの強みを最大限に活かせる仕組みを提案します!【提案】①ハウス不動産に専門担当(情報を秘匿できる方)を配置②入居審査廃止③前入居者退去後のクリーニング・修理を優先的対応し希望入居日早期確定④引越業者提携による単身赴任者専用窓口で優先対応\n",
      "ESG: 7.992933236943242e-05\n",
      "サービス: 1.0882560527800038e-05\n",
      "商品開発: 3.841732125281841e-07\n",
      "営業: 0.02882151259442405\n",
      "新規事業: 2.3311204071820658e-07\n",
      "施工: 0.3518878485058691\n",
      "生産: 0.005767994719799538\n",
      "福利厚生・制度: 0.04163899887490268\n",
      "総務: 0.567699185612459\n",
      "設計: 0.00409303051436909\n",
      "---\n",
      "地鎮祭日程が決まったら、日程と必要図面を自動で関係者へ発信してくれる機能。\n",
      "また、地鎮祭予定の１４日前にこのシステムが使用されていなければ、関係者へ注意の発信が送られる\n",
      "ESG: 0.004555305310027957\n",
      "サービス: 0.0017094569929446363\n",
      "商品開発: 0.0002922415930220122\n",
      "営業: 0.06812639360863784\n",
      "新規事業: 0.00026320746077575225\n",
      "施工: 0.3000369039573317\n",
      "生産: 0.018754582975748318\n",
      "福利厚生・制度: 0.14248067888028898\n",
      "総務: 0.41026664859487416\n",
      "設計: 0.05351458062635697\n",
      "---\n",
      "解体現場は着工前にお客様がやるべき事が多数あります。仮住まいへの引越し、家財の処分検討、ライフライン切断連絡…。これらを全て当社手配で行えないでしょうか？引越業者やリサイクル業者と提携し、ご希望を聞いてあとはお任せ！ライフライン対応も代行出来ればお任せ！\n",
      "着工前の施主様の御負担軽減及び当社監督の業務削減につながると思います。\n",
      "\n",
      "ESG: 0.00038883780772514784\n",
      "サービス: 6.852025441032634e-05\n",
      "商品開発: 2.7969166789643686e-06\n",
      "営業: 0.05032200555228882\n",
      "新規事業: 2.7912894725307633e-06\n",
      "施工: 0.2873111942132559\n",
      "生産: 0.011420648998469524\n",
      "福利厚生・制度: 0.09762036647019293\n",
      "総務: 0.5474751867409235\n",
      "設計: 0.005387651756560805\n",
      "---\n",
      "支店内で登山部を結成して活動する事で社員の健康促進とコミュニケーションの活性化にも寄与する事が出来ます。\n",
      "登山を楽しむ事がウォーキングチャレンジのスコアアップになります。\n",
      "全社にいる登山愛好家の皆様と繋がる事で支店や本部を超えた活動に発展する可能性もあり、各地域の山の良さを全社に向けて発信する事で地方創生活動への足掛かりになると考えています。\n",
      "ESG: 5.4052764861520924e-05\n",
      "サービス: 2.013718158843207e-06\n",
      "商品開発: 9.482087606881707e-08\n",
      "営業: 0.022448034648893597\n",
      "新規事業: 3.809382647015233e-08\n",
      "施工: 0.12338407194081405\n",
      "生産: 0.0018943304729509096\n",
      "福利厚生・制度: 0.045333411279747576\n",
      "総務: 0.8059132631793129\n",
      "設計: 0.0009706890805848818\n",
      "---\n",
      "深刻な空き家問題。解決が進まない理由の一つに、「使いにくい敷地が多い」が考えられます。分筆を繰り返した区画、虫食い状の区画では売る側も買う側も困りもの。\n",
      "でも、積水ハウスグループの力なら解決できると思いませか？\n",
      "空き家周辺の土地を含めた区画内整理で、使いやすい敷地にグレードアップ！土地活用の幅も広がり、街の価値向上につながります。\n",
      "行政介入しにくい分野を当社が率先して事業の確立していきましょう！\n",
      "ESG: 0.00012565604374693915\n",
      "サービス: 1.1966043360710752e-05\n",
      "商品開発: 5.415184727760761e-07\n",
      "営業: 0.03433706454183673\n",
      "新規事業: 4.024201296420616e-07\n",
      "施工: 0.3651990040069078\n",
      "生産: 0.010326690073592174\n",
      "福利厚生・制度: 0.039314572006051436\n",
      "総務: 0.5424915935243639\n",
      "設計: 0.008192509821523442\n",
      "---\n",
      "ライフニットデザインのサンプル手配の効率化。\n",
      "増え続けるサンプルのメンテナンスはICにとって負担がとても大きいのです。\n",
      "本社で一括管理をお願いできれば、発注漏れの解消とICの業務効率UPに繋がります。\n",
      "ESG: 0.007885688101735854\n",
      "サービス: 0.00301691846178618\n",
      "商品開発: 0.0007638530455913223\n",
      "営業: 0.08246139322447818\n",
      "新規事業: 0.0005641452458694157\n",
      "施工: 0.30058104408587055\n",
      "生産: 0.019211039703638785\n",
      "福利厚生・制度: 0.15830061889554298\n",
      "総務: 0.3847462171813015\n",
      "設計: 0.04246908205418439\n",
      "---\n",
      "浮き沈みのある心を持つ方々は、どのような住環境だと気分が安らぐのか？\n",
      "自閉症者を含む発達障がい者、認知症の方々等が心穏やかに暮らせる住環境を提供できれば、いずれの人にとっても心穏やかな住まいを提供できるのではないでしょうか。\n",
      "感受性が強い（感性が豊か過ぎる）がゆえ生活しづらさを抱えている人から学び、物理的な配慮だけでなく心理的な配慮もある、五感全てに心地よい感性豊かな住まいを創りませんか\n",
      "\n",
      "ESG: 0.0004121573086599172\n",
      "サービス: 3.5616392746817415e-05\n",
      "商品開発: 3.670970495854154e-06\n",
      "営業: 0.054554460069679074\n",
      "新規事業: 7.382757183876993e-07\n",
      "施工: 0.12244241717981397\n",
      "生産: 0.002344927399372378\n",
      "福利厚生・制度: 0.0800846979491311\n",
      "総務: 0.7378068312878179\n",
      "設計: 0.002314483166542789\n",
      "---\n",
      "職場が古くて汚い、書類が山積み・・・それを当たり前にしたくない！\n",
      "「オフィスリノベーション」\n",
      "フリーアドレスにて業務を改善。最適緑視率４％を指標にストレスフリーな環境の実現。\n",
      "多様な用途の空間を作ることで、遊ぶように働く場所生む。\n",
      "これまでの凝り固まった発想から脱却し、未来を描ける職場を目指します！\n",
      "ESG: 0.003918320954932663\n",
      "サービス: 0.0010368745919418934\n",
      "商品開発: 0.00025558070506933705\n",
      "営業: 0.059557938560200654\n",
      "新規事業: 0.0001629897443925621\n",
      "施工: 0.369657587425709\n",
      "生産: 0.019581342468871277\n",
      "福利厚生・制度: 0.12872543102515738\n",
      "総務: 0.38490458172921865\n",
      "設計: 0.03219935279450207\n",
      "---\n",
      "全国の建設社員とアプリを通して楽しくコミュニケーションを取り、ゲーム感覚で楽しみながら現場管理・仕事が可能！お互いの成長・スキルアップ・離職率低減が目的の社内用ＳＮＳアプリです！！\n",
      "\n",
      "ESG: 0.013962423150444598\n",
      "サービス: 0.005590457380821527\n",
      "商品開発: 0.001019551045454501\n",
      "営業: 0.09197136527461272\n",
      "新規事業: 0.0018293883385827273\n",
      "施工: 0.30682366445418885\n",
      "生産: 0.01555953833275276\n",
      "福利厚生・制度: 0.19606271983999574\n",
      "総務: 0.318337095320879\n",
      "設計: 0.0488437968622672\n",
      "---\n",
      "レベチ（レベルが違う）と感じるすごい社員を紹介し合い、探す。業務スキルでも、ニッチな分野でも趣味でもレベルの高い人との関係づくりが多様で強い組織づくりになる。高度なリソース（経験、知識、スキル、趣味など）を知り、AI時代だからこそ、人のパフォーマンスや好奇心に目を向け、夢を持てるようになる。そんなイノベーション＆コミュニケーションを目指しませんか？\n",
      "\n",
      "ESG: 0.0012674891016128567\n",
      "サービス: 0.0001611474766998817\n",
      "商品開発: 1.957405211514936e-05\n",
      "営業: 0.052041252391213676\n",
      "新規事業: 1.3634570558599104e-05\n",
      "施工: 0.2809367740991434\n",
      "生産: 0.006504163023459087\n",
      "福利厚生・制度: 0.09204175610157765\n",
      "総務: 0.5519399384100108\n",
      "設計: 0.015074270773618413\n",
      "---\n",
      "未だ距離がある住宅と医療の間を具体的に縮める、医住連携実現に向けた業界初の取組み。\n",
      "家庭内での「医薬管理メソッド」 と「新たな収納システム」の、住生活目線での一体的な提案。\n",
      "家族の健康に関するモノ、情報を一括管理できる収納（ハード）や、健やかな日を増やす住まい方（ソフト）提案、家族全員の健康リテラシーを高めるサービスで構成。\n",
      "ESG: 0.0009123454578069732\n",
      "サービス: 0.00027421673674357633\n",
      "商品開発: 2.0858382623552175e-05\n",
      "営業: 0.05602364901271566\n",
      "新規事業: 1.9889207202717602e-05\n",
      "施工: 0.3760329248379264\n",
      "生産: 0.01125836975136788\n",
      "福利厚生・制度: 0.09077122309306103\n",
      "総務: 0.44751713374182145\n",
      "設計: 0.017169389778745252\n",
      "---\n",
      "年々気温や紫外線の量が増えているのが現状。紫外線の量が増えれば、急性傷害・慢性傷害のリスクが増える。そこで、紫外線を防ぐために日焼け止めして現場に行こう。\n",
      "具体的に、6月～9月の期間中、1000円を上限にコンカー申請が可能に。※1000円以上の場合は差額分を自己負担とする。結果、紫外線による日焼け等の皮膚疾患が減少。現場で働く女性が増える。男性も美白を意識する時代。今より営業活動に励む事が出来る。\n",
      "ESG: 0.00011110940607665612\n",
      "サービス: 7.141675234049406e-06\n",
      "商品開発: 1.2749728278048919e-06\n",
      "営業: 0.02878196672755072\n",
      "新規事業: 1.5874119036116511e-07\n",
      "施工: 0.3037058997707243\n",
      "生産: 0.00852950275753208\n",
      "福利厚生・制度: 0.0529017176620455\n",
      "総務: 0.6019143682017517\n",
      "設計: 0.004046860085043581\n",
      "---\n",
      "My STAGEは現在営業がメインで使用するツールですが、設計、IC、現場監督、カスタマーズセンターまで一貫して使えるものに発展してはどうでしょうか。\n",
      "打合せの議事録、最新図面、見積、設備図面等、お客様はタイムリーにご確認ができる。変更連絡等、現在LINE WORKSやメールでのやりとりも、そこで行えば担当者と共有ができます。カスタマーズセンターも打合せ当時の情報を確認できると良いと思います。\n",
      "ESG: 3.5989623291353084e-05\n",
      "サービス: 4.170399697174072e-06\n",
      "商品開発: 1.4637282111519221e-07\n",
      "営業: 0.02626513001570008\n",
      "新規事業: 6.519067658766072e-08\n",
      "施工: 0.25566683362802023\n",
      "生産: 0.0052485656955334236\n",
      "福利厚生・制度: 0.03421574187977618\n",
      "総務: 0.6723624979507217\n",
      "設計: 0.006200859243736875\n",
      "---\n",
      "建物の引き渡し後もオーナーさまとお付き合いが続くように積水ハウスが提携するメーカーさんと共同でインテリアのサブスクサービスの提供をしてはいかがでしょうか。\n",
      "自分好みや季節にあったインテリアなどを自由にレンタルでき、コーディネーターの方にも相談が可能なサービス。展示品なども活用したり、気に入ればインテリアがあれば購入も可能。\n",
      "引き渡し後も幸福感、居心地のいい家を提供し続けられるのではないでしょうか。\n",
      "ESG: 0.0006724970701168345\n",
      "サービス: 0.0001968528891325448\n",
      "商品開発: 7.598758072270498e-06\n",
      "営業: 0.0922351685382274\n",
      "新規事業: 9.046685649732365e-06\n",
      "施工: 0.20501131514076212\n",
      "生産: 0.005503624000005865\n",
      "福利厚生・制度: 0.10067603994784778\n",
      "総務: 0.5887822142795895\n",
      "設計: 0.006905642690581221\n",
      "---\n",
      "新たに入社された方がすごく優秀でも地域社員入社の場合は３年間の勤続年数が必要になります。\n",
      "その間にその人がすごく優秀でも給料は総合職や専門職の方より下回ってしまいます。\n",
      "そうなると給料のよい所や、待遇がいい所に流出してまい改善できるのに出来ない環境になり悪循環になりえると思います。\n",
      "優秀な人は即時で昇格昇給をできるようにしませんか？\n",
      "ESG: 5.3821061247569176e-05\n",
      "サービス: 2.553232259242885e-06\n",
      "商品開発: 1.6148644491697756e-07\n",
      "営業: 0.020325373534307148\n",
      "新規事業: 3.947194822853769e-08\n",
      "施工: 0.10790297877303925\n",
      "生産: 0.002263592896171854\n",
      "福利厚生・制度: 0.07518965230034293\n",
      "総務: 0.793265880273774\n",
      "設計: 0.0009959469704605573\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "res = rlf.predict_proba(\"ホワイトボードに在席状況を入力して、他部署からも確認できるようにしたい。\")\n",
    "\n",
    "y_pred = [rlf.predict_proba(doc) for doc in X_test]\n",
    "\n",
    "count = 0\n",
    "for X_test, y_pred in zip(X_test, y_pred):\n",
    "    print(X_test)\n",
    "    for cat, y in zip(rlf.categories, y_pred): \n",
    "        print(cat + \": \" + str(y))\n",
    "    print(\"---\")\n",
    "\n",
    "    count += 1\n",
    "    if count > 20:\n",
    "        break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shipai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
